{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T19:29:48.099290Z",
     "start_time": "2025-08-13T19:29:36.109173Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"akdeniz27/bert-base-turkish-cased-ner\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"akdeniz27/bert-base-turkish-cased-ner\")\n",
    "ner = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "ner(\"Metin burada girilir\")\n",
    "\n",
    "\n",
    "\n",
    "dbmdz/distilbert-base-turkish-cased\n",
    "\"\"\"\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          DataCollatorForTokenClassification, TrainingArguments, Trainer)\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "label_list = [\"O\",\"B-IL\",\"I-IL\",\"B-ILCE\",\"I-ILCE\",\"B-MAH\",\"I-MAH\",\"B-SOK\",\"I-SOK\",\"B-NO\",\"I-NO\",\"B-PK\",\"I-PK\"]\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}"
   ],
   "id": "e40a3076258f7484"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/Users/bariscakmak/Desktop/localhost/playground/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      3\u001B[39m model = AutoModelForTokenClassification.from_pretrained(\n\u001B[32m      4\u001B[39m     model_name, num_labels=\u001B[38;5;28mlen\u001B[39m(label_list), id2label=id2label, label2id=label2id\n\u001B[32m      5\u001B[39m )\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Örnek: Hugging Face Datasets formatında token/labels kolonları olduğunu varsayalım\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m dataset = \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mjson\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain.json\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mvalidation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mvalid.json\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/localhost/venv/lib/python3.13/site-packages/datasets/load.py:1392\u001B[39m, in \u001B[36mload_dataset\u001B[39m\u001B[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001B[39m\n\u001B[32m   1387\u001B[39m verification_mode = VerificationMode(\n\u001B[32m   1388\u001B[39m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode.BASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode.ALL_CHECKS\n\u001B[32m   1389\u001B[39m )\n\u001B[32m   1391\u001B[39m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1392\u001B[39m builder_instance = \u001B[43mload_dataset_builder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1393\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1394\u001B[39m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1395\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1396\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1397\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1398\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1399\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1400\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1401\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1402\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1403\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1404\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1405\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1407\u001B[39m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[32m   1408\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/localhost/venv/lib/python3.13/site-packages/datasets/load.py:1132\u001B[39m, in \u001B[36mload_dataset_builder\u001B[39m\u001B[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001B[39m\n\u001B[32m   1130\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1131\u001B[39m     features = _fix_for_backward_compatible_features(features)\n\u001B[32m-> \u001B[39m\u001B[32m1132\u001B[39m dataset_module = \u001B[43mdataset_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1133\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1134\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1135\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1136\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1137\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1139\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1140\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1141\u001B[39m \u001B[38;5;66;03m# Get dataset builder class\u001B[39;00m\n\u001B[32m   1142\u001B[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/localhost/venv/lib/python3.13/site-packages/datasets/load.py:912\u001B[39m, in \u001B[36mdataset_module_factory\u001B[39m\u001B[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001B[39m\n\u001B[32m    889\u001B[39m \u001B[38;5;66;03m# We have several ways to get a dataset builder:\u001B[39;00m\n\u001B[32m    890\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    891\u001B[39m \u001B[38;5;66;03m# - if path is the name of a packaged dataset module\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    903\u001B[39m \n\u001B[32m    904\u001B[39m \u001B[38;5;66;03m# Try packaged\u001B[39;00m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES:\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPackagedDatasetModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    907\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    908\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    909\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    910\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    911\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m--> \u001B[39m\u001B[32m912\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    913\u001B[39m \u001B[38;5;66;03m# Try locally\u001B[39;00m\n\u001B[32m    914\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m path.endswith(filename):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/localhost/venv/lib/python3.13/site-packages/datasets/load.py:526\u001B[39m, in \u001B[36mPackagedDatasetModuleFactory.get_module\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    520\u001B[39m base_path = Path(\u001B[38;5;28mself\u001B[39m.data_dir \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m).expanduser().resolve().as_posix()\n\u001B[32m    521\u001B[39m patterns = (\n\u001B[32m    522\u001B[39m     sanitize_patterns(\u001B[38;5;28mself\u001B[39m.data_files)\n\u001B[32m    523\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.data_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    524\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m get_data_patterns(base_path, download_config=\u001B[38;5;28mself\u001B[39m.download_config)\n\u001B[32m    525\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m526\u001B[39m data_files = \u001B[43mDataFilesDict\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    527\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    528\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    529\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    530\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    532\u001B[39m module_path, \u001B[38;5;28mhash\u001B[39m = _PACKAGED_DATASETS_MODULES[\u001B[38;5;28mself\u001B[39m.name]\n\u001B[32m    534\u001B[39m builder_kwargs = {\n\u001B[32m    535\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdata_files\u001B[39m\u001B[33m\"\u001B[39m: data_files,\n\u001B[32m    536\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdataset_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m.name,\n\u001B[32m    537\u001B[39m }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/localhost/venv/lib/python3.13/site-packages/datasets/data_files.py:689\u001B[39m, in \u001B[36mDataFilesDict.from_patterns\u001B[39m\u001B[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[39m\n\u001B[32m    684\u001B[39m out = \u001B[38;5;28mcls\u001B[39m()\n\u001B[32m    685\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m key, patterns_for_key \u001B[38;5;129;01min\u001B[39;00m patterns.items():\n\u001B[32m    686\u001B[39m     out[key] = (\n\u001B[32m    687\u001B[39m         patterns_for_key\n\u001B[32m    688\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(patterns_for_key, DataFilesList)\n\u001B[32m--> \u001B[39m\u001B[32m689\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mDataFilesList\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    690\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpatterns_for_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    691\u001B[39m \u001B[43m            \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    692\u001B[39m \u001B[43m            \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    693\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    694\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    695\u001B[39m     )\n\u001B[32m    696\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/localhost/venv/lib/python3.13/site-packages/datasets/data_files.py:582\u001B[39m, in \u001B[36mDataFilesList.from_patterns\u001B[39m\u001B[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[39m\n\u001B[32m    579\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m patterns:\n\u001B[32m    580\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    581\u001B[39m         data_files.extend(\n\u001B[32m--> \u001B[39m\u001B[32m582\u001B[39m             \u001B[43mresolve_pattern\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    583\u001B[39m \u001B[43m                \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    584\u001B[39m \u001B[43m                \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    585\u001B[39m \u001B[43m                \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    586\u001B[39m \u001B[43m                \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    587\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    588\u001B[39m         )\n\u001B[32m    589\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[32m    590\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_magic(pattern):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/localhost/venv/lib/python3.13/site-packages/datasets/data_files.py:383\u001B[39m, in \u001B[36mresolve_pattern\u001B[39m\u001B[34m(pattern, base_path, allowed_extensions, download_config)\u001B[39m\n\u001B[32m    381\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m allowed_extensions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    382\u001B[39m         error_msg += \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m with any supported extension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(allowed_extensions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m383\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(error_msg)\n\u001B[32m    384\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[31mFileNotFoundError\u001B[39m: Unable to find '/Users/bariscakmak/Desktop/localhost/playground/train.json'"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "model_name = \"dbmdz/bert-base-turkish-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# Örnek: Hugging Face Datasets formatında token/labels kolonları olduğunu varsayalım\n",
    "dataset = load_dataset(\"json\", data_files={\"train\":\"train.json\",\"validation\":\"valid.json\"})"
   ],
   "id": "a143531223cc457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "def tokenize_align(example):\n",
    "    toks = tokenizer(example[\"tokens\"], is_split_into_words=True,\n",
    "                     truncation=True, padding=False, max_length=128)\n",
    "    word_ids = toks.word_ids()\n",
    "    labels = []\n",
    "    i = 0\n",
    "    for wid in word_ids:\n",
    "        if wid is None:\n",
    "            labels.append(-100)                 # loss'a katılmasın\n",
    "        else:\n",
    "            labels.append(example[\"labels\"][wid])\n",
    "    toks[\"labels\"] = labels\n",
    "    return toks"
   ],
   "id": "4b0c4f4041ae3b7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m dataset = \u001B[43mdataset\u001B[49m.map(tokenize_align, batched=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m      2\u001B[39m data_collator = DataCollatorForTokenClassification(tokenizer)\n\u001B[32m      4\u001B[39m args = TrainingArguments(\n\u001B[32m      5\u001B[39m     output_dir=\u001B[33m\"\u001B[39m\u001B[33mner_tr_model\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      6\u001B[39m     learning_rate=\u001B[32m2e-5\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m     fp16=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     14\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "dataset = dataset.map(tokenize_align, batched=False)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ner_tr_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args,\n",
    "                  train_dataset=dataset[\"train\"],\n",
    "                  eval_dataset=dataset[\"validation\"],\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=data_collator)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "b3a4091bcbb8fc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "425ad5cff0434724"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
